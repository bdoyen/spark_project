{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and environement setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages loading\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, Word2Vec, HashingTF, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark env setting\n",
    "sc = SparkContext()\n",
    "# spark = (SparkSession.builder.getOrCreate())\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"PLP Yelp\")\n",
    "    .enableHiveSupport()\n",
    "    .config(\"spark.executor.memory\",\"8G\")\n",
    "    .config(\"spark.driver.memory\",\"18G\")\n",
    "    .config(\"spark.executor.cores\",\"6\")\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 22.28649401664734 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Full data loading\n",
    "\n",
    "start_time = time.time()\n",
    "comments = spark.read.json(\"yelp_academic_dataset_review.json\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of total comments\n",
    "comments.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.054257869720458984 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# reducing the dataset\n",
    "start_time = time.time()\n",
    "comments_string = comments.select('stars','text')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.013917922973632812 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Sub-setting the initial dataset - 10% sampling\n",
    "\n",
    "start_time = time.time()\n",
    "comments_string = comments_string.sample(False,0.02,0)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.13809585571289062 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Splitting text into words using regex matching\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"text_cleaned\", pattern=\"\\\\W\")\n",
    "start_time = time.time()\n",
    "comments_words = regexTokenizer.transform(comments_string)\n",
    "comments_words = comments_words.select('stars', 'text_cleaned')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another type of splitting\n",
    "\n",
    "#start_time = time.time()\n",
    "#comments_words = comments_string_clened.withColumn(\n",
    "#        \"text_cleaned\",\n",
    "#        split(col(\"text_cleaned\"), \" \").cast(\"array<string>\").alias(\"ev\")\n",
    "# )\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.2830619812011719 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# remove stop-words\n",
    "start_time = time.time()\n",
    "remover = StopWordsRemover(inputCol=\"text_cleaned\", outputCol=\"words\")\n",
    "comments_words = remover.transform(comments_words)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.010272026062011719 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "comments_words = comments_words.select(\"stars\",\"words\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.062242746353149414 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Stemming : less words, more meaning\n",
    "\n",
    "# Import stemmer library\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# Instantiate stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create stemmer python function\n",
    "def stem(in_vec):\n",
    "    out_vec = []\n",
    "    for t in in_vec:\n",
    "        t_stem = stemmer.stem(t)\n",
    "        if len(t_stem) > 2:\n",
    "            out_vec.append(t_stem)       \n",
    "    return out_vec\n",
    "\n",
    "# Create user defined function for stemming with return type Array<String>\n",
    "stemmer_udf = udf(lambda x: stem(x), ArrayType(StringType()))\n",
    "\n",
    "# Create new df with vectors containing the stemmed tokens \n",
    "start_time = time.time()\n",
    "df_final = (\n",
    "    comments_words\n",
    "        .withColumn(\"words_stemmed\", stemmer_udf(\"words\"))\n",
    "        .select(\"stars\",\"words_stemmed\")\n",
    "  )\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-visualization\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 : BoW + Bayesian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0, test0 = df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Apply Bag-of-words transformation\n",
    "vectorizer = CountVectorizer(inputCol=\"words_stemmed\", outputCol=\"bag_of_words\")\n",
    "\n",
    "# Convert string labels to floats\n",
    "label_indexer = StringIndexer(inputCol=\"stars\", outputCol=\"label_index\")\n",
    "\n",
    "# Creating Pipeline\n",
    "pipeline0 = Pipeline(stages=[vectorizer, label_indexer])\n",
    "\n",
    "# Applying Pipeline to intitial dataset\n",
    "start_time = time.time()\n",
    "pipelineFit0 = pipeline0.fit(train0)\n",
    "train_bag_of_words = pipelineFit0.transform(train0)\n",
    "test_bag_of_words = pipelineFit0.transform(test0)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_bag_of_words.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn multiclass classifier on training data\n",
    "start_time = time.time()\n",
    "classifier = NaiveBayes(\n",
    "    labelCol=\"label_index\", featuresCol=\"bag_of_words\", predictionCol=\"label_index_predicted\"\n",
    ")\n",
    "classifier_transformer = classifier.fit(train_bag_of_words)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on test data\n",
    "start_time = time.time()\n",
    "test_predicted = classifier_transformer.transform(test_bag_of_words)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier evaluation\n",
    "start_time = time.time()\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label_index\", predictionCol=\"label_index_predicted\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(test_predicted)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 : TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "train1, test1 = df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Add HashingTF and IDF to transformation\n",
    "hashingTF = HashingTF(inputCol=\"words_stemmed\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=3) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"stars\", outputCol = \"label\")\n",
    "\n",
    "# Creating Pipeline\n",
    "pipeline1 = Pipeline(stages=[hashingTF, idf, label_stringIdx])\n",
    "\n",
    "# Applying Pipeline to intitial dataset\n",
    "start_time = time.time()\n",
    "pipelineFit1 = pipeline1.fit(train1)\n",
    "train_idf = pipelineFit1.transform(train1)\n",
    "test_idf = pipelineFit1.transform(test1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "lr1 = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "start_time = time.time()\n",
    "lrModel1 = lr1.fit(train_idf)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction on test\n",
    "start_time = time.time()\n",
    "predictions1 = lrModel1.transform(test_idf)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Evaluation on test\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "start_time = time.time()\n",
    "accuracy1 = evaluator.evaluate(predictions1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 : Word2Vec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "train2, test2 = df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Word2Vec computation\n",
    "word2Vec = Word2Vec(vectorSize=300, minCount=5, inputCol=\"words_stemmed\", outputCol=\"result\")\n",
    "start_time = time.time()\n",
    "model = word2Vec.fit(train2)\n",
    "train_w2v = model.transform(train2)\n",
    "test_w2v = model.transform(test2)\n",
    "# dataset formatting\n",
    "trainingData = train_w2v.select('stars','result')\n",
    "testingData = test_w2v.select('stars','result')\n",
    "trainingData = trainingData.withColumnRenamed(\"stars\", \"label\").withColumnRenamed(\"result\", \"features\")\n",
    "testingData = testingData.withColumnRenamed(\"stars\", \"label\").withColumnRenamed(\"result\", \"features\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "# Train model with Training Data\n",
    "start_time = time.time()\n",
    "lrModel = lr.fit(trainingData)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on Test\n",
    "start_time = time.time()\n",
    "predictions = lrModel.transform(testingData)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on Test\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "start_time = time.time()\n",
    "accuracy2 = evaluator.evaluate(predictions)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(\"Accuracy = {:.2f}\".format(accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deep : TF-IDF + LSTM with Elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test split\n",
    "train3, test3 = df_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Add HashingTF and IDF to transformation\n",
    "hashingTF = HashingTF(inputCol=\"words_stemmed\", outputCol=\"rawFeatures\", numFeatures=300)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=3) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"stars\", outputCol = \"label\")\n",
    "\n",
    "# Creating Pipeline\n",
    "pipeline3 = Pipeline(stages=[hashingTF, idf, label_stringIdx])\n",
    "\n",
    "# Applying Pipeline to intitial dataset\n",
    "start_time = time.time()\n",
    "pipelineFit3 = pipeline3.fit(train3)\n",
    "train_idf = pipelineFit3.transform(train3)\n",
    "test_idf = pipelineFit3.transform(test3)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idf = train_idf.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "slen = udf(lambda s: len(set(s)), IntegerType())\n",
    "\n",
    "df2 = train_idf.withColumn(\"features\", slen(train_idf.features))\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 32  # number of hidden units in the LSTM\n",
    "vocab_size = 1000  # size of the vocabulary\n",
    "nb_classes = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(nb_classes, activation='sigmoid'))\n",
    "loss_classif     =  'categorical_crossentropy' \n",
    "optimizer        =  'adam' \n",
    "metrics_classif  =  ['accuracy']\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "\n",
    "rdd = train_idf.rdd.map(lambda x:x.stringFieldName.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephas.spark_model import SparkModel\n",
    "\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Deep :  without Spark... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = df_final.randomSplit([0.8, 0.2])\n",
    "pandas_df_train = train.toPandas()\n",
    "pandas_df_test = test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = pandas_df_train['words_stemmed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings_test = pandas_df_test['words_stemmed'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_data = set(x for l in strings for x in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [' '.join(sentence) for sentence in strings]\n",
    "data_test = [' '.join(sentence) for sentence in strings_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "#config = tf.ConfigProto(device_count={\"CPU\": 6})\n",
    "#keras.backend.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_train = len(unique_data)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size_train)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "train_data = tokenizer.texts_to_sequences(data_train)\n",
    "dev_data = tokenizer.texts_to_sequences(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = max([len(elem) for elem in train_data])\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(train_data, maxlen=maxlen, dtype='int32', \n",
    "                                                               padding='pre', truncating='pre', value=0.0)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(dev_data, maxlen=maxlen, dtype='int32', \n",
    "                                                               padding='pre', truncating='pre', value=0.0)\n",
    "\n",
    "trainy = pandas_df_train['stars'].tolist()\n",
    "valy = pandas_df_test['stars'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 669)         37624560  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                89856     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 37,714,581.0\n",
      "Trainable params: 37,714,581\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = x_train.shape[1]  # word embedding dimension\n",
    "nhid       = 32  # number of hidden units in the LSTM\n",
    "vocab_size = vocab_size_train  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "n_epochs = 4\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
